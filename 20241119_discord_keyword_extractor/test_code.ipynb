{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 명사 추출 모델 비교  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|모델|설명|강점|한글|영문|기타언어|URL|\n",
    "|---|---|---|---|---|---|---|\n",
    "|OKT|Konlpy 라이브러리에 포함된 형태소 분석기. Open Korean Text. 원래 트위터의 한국어 텍스트 분석을 위해 개발되었으며, Konlpy에서도 Twitter 형태소 분석기로 활용할 수 있습니다. 캐주얼한 언어(예: 줄임말, 비속어 등)에 대한 분석에 강점이 있으며, 트위터 및 소셜 미디어 텍스트 분석에 자주 사용됩니다.|트위터 기반의 캐주얼한 텍스트에 강함, 비정형 텍스트(줄임말, 비속어 등) 분석에 우수. 빠른 속도.|O|제한적|제한적|https://github.com/open-korean-text/open-korean-text|\n",
    "|Hannanum|Konlpy 라이브러리에 포함된 형태소 분석기. 한국전자통신연구원(ETRI)에서 개발한 형태소 분석기로, 품사 태깅에 강점이 있습니다.|품사 태깅에 강점. 품질이 높은 형태소 분석기|O|X|X|https://konlpy-ko.readthedocs.io/ko/v0.4.3/|\n",
    "|Komoran|Konlpy 라이브러리에 포함된 형태소 분석기. 코모란은 검색 엔진 최적화와 같은 환경에서 주로 사용되는 분석기입니다.|검색 엔진 최적화 환경에 특화, 속도가 빠르고 정확도가 높은 한국어 전용 분석기|O|X|X|https://konlpy-ko.readthedocs.io/ko/v0.4.3/|\n",
    "|Kkma|Konlpy 라이브러리에 포함된 형태소 분석기. 서울대학교에서 개발한 분석기로, 학술 연구에 많이 사용됩니다.|학술 연구에 적합, 정밀한 분석 가능, 서울대학교에서 개발하여 신뢰도가 높음|O|X|X|https://konlpy-ko.readthedocs.io/ko/v0.4.3/|\n",
    "|Khaiii(카이)|카카오는 자체 개발한 형태소 분석기인 Khaiii를 공개했으며, 기존의 형태소 분석기보다 속도가 빠르고 정확도가 높다고 평가받습니다. 텐서플로우 기반의 딥러닝 모델을 사용하여 높은 성능을 자랑하며, 특히 한국어 명사 추출과 구문 분석에서 좋은 결과를 보입니다.|딥러닝 기반의 높은 정확도, 빠른 속도, 한국어 고유의 문법 및 언어적 특성에 최적화|O|X|X|https://github.com/kakao/khaiii|\n",
    "|Mecab(메캅)|Mecab은 원래 일본어 형태소 분석기로 개발되었으나, 한국어를 지원하는 버전도 공개되었습니다. 매우 빠른 속도와 가벼운 성능을 자랑하며, 특히 모바일 환경과 같은 자원이 제한된 환경에서 자주 사용됩니다.|가벼운 성능과 빠른 속도, 모바일 환경에 적합, 일본어와 한국어 모두 지원 가능|O|X|일본어||\n",
    "|Stanza|Stanford NLP에서 개발한 다국어 NLP 툴킷으로, 형태소 분석과 품사 태깅, 구문 분석이 가능.<br>사용 목적: 다국어 형태소 분석, 품사 태깅, 구문 분석<br>특징: 60개 이상의 언어 지원, 한국어와 영어의 형태소 분석 및 품사 태깅 가능<br>단점: 일부 언어에 대한 지원이 다소 부족할 수 있으며, 특정 언어에 대한 세부적인 맞춤화는 제한적||O|O|O(60개 이상)||\n",
    "|NLTK|사용 목적: 영어 텍스트 분석, 품사 태깅, 텍스트 전처리<br>특징: 영어에 특화된 다양한 기능 제공, 토크나이저, 품사 태깅, 구문 분석 등 기능이 매우 방대<br>단점: 한국어와 다른 언어 지원이 제한적, 한국어 분석은 별도의 플러그인 설치 필요|방대한 영어 텍스트 분석 라이브러리, 토크나이저와 품사 태깅, 파싱 등 NLP 전반 지원|제한적(플러그인)|O|제한적||\n",
    "|SpaCy|사용 목적: 영어 및 유럽 언어 텍스트 분석, 토크나이징, 품사 태깅, 네임드 엔티티 인식(NER)<br>특징: 뛰어난 속도와 성능, 플러그인을 통해 KoNLPy와 연동 가능, 유럽 언어에 최적화<br>단점: 한국어 지원이 제한적이므로, 다국어 텍스트 분석 시 언어 분리 후 처리 필요|뛰어난 성능과 속도, 플러그인으로 KoNLPy와 연동 가능, 유럽 언어에 최적화되어 있으나 다국어 확장 가능|제한적(플러그인)|O|O(20개 이상)||\n",
    "|Multilingual BERT|사용 목적: 다국어 텍스트 분석, 문장 임베딩 생성, 번역, 텍스트 분류 등.<br>특징: 트랜스포머 기반으로 강력한 언어 이해 능력, 한국어와 영어 텍스트에 대한 공동 임베딩 생성 가능<br>단점: 고사양의 GPU 필요, 형태소 단위 분석보다는 문장 임베딩에 최적화|100개 이상의 언어를 지원하는 강력한 다국어 모델, 문장 및 단어 임베딩 생성에 탁월, 트랜스퍼 러닝 가능|O|O|O(100개 이상)||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 테스트 텍스트 데이터  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_path = os.getcwd().replace('\\\\', '/')\n",
    "dir_path = '/test_text'\n",
    "test_files = [root_path + dir_path + '/' + x for x in os.listdir(root_path + dir_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Konlpy - OKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ## 최근접이웃 분류기의 개념   ### 최근접이웃 분류기의 개념   ![](/assets/images/20241117_001_001.png)   분류 문제를 생각해봤을 때, 간단한 분류 방법으로는 `새로 주어진 데이터 xnew에 대해 거리가 가장 가까운 데이터를 식별하고, 식별된 근접 데이터의 클래스를 xnew으 클래스로 할당한다.` 와 같이, 근처에 있는 데이터와 동일한 클래스로 분류하는 방법을 생각해 볼 수 있을 것이다. 이것이 바로 최근접이웃 분류기의 개념이다.   ### 최근접이웃 분류기의 수식화     데이터 x와 x1간 거리를 `d(x, x1)`이라고 했을 때 x의 클래스를 결정짓는 함수 즉, 최근접이웃 분류기의 결정규칙을 식으로 표현하면 아래와 같다.   ![](/assets/images/20241117_001_002.png)   ### 최근접이웃 분류기의 처리과정   |순서|단계|설명| |---|---|---| |1|거리 계산|새로운 데이터 x와 모든 학습 데이터 {x1, x2, ... xn}과의 거리를 계산한다.| |2|최근접 이웃 식별|거리가 가장 가까운 데이터 xmin을 식별한다.| |3|클래스 할당|새로운 데이터 x에 대해 xmin의 클래스와 같은 클래스를 할당하도록 결정한다.| ### 최근접이웃 분류기의 문제점   이러한 최근접이웃 분류기는 문제점을 가지고 있는데 그것은 (1)데이터 잡음에 민감하다는 것과 (2)과다적합이 발생한다는 점이다.   |문제점|설명| |---|---| |데이터 잡음에 민감|- 데이터 잡음이란, 이상치와 같이 일반적이지 않은 데이터를 의미한다.<br>- 최근접이웃 분류기는 가장 가까운 \"하나의 데이터\"만을 참고하기 때문에, 참고하는 데이터가 이상치인 경우에는 오분류가 발생할 가능성이 높다.| |과다적합|- 과다적합 또는 과적합이라고 부르는 것은, 분류 모델이 학습 데이터에만 너무 적합하여 현실 세계 혹은 테스트 데이터에서 분류율이 떨어지는 문제를 말한다.<br>- 앞서 살펴본 데이터 잡음에 민감하다는 문제와 일정 부분 비슷한 면이 있다.| ![](/assets/images/20241117_001_003.png)   |구분|KNN (K=1)|베이즈 분류기| |---|---|---| |학습 정확도|1.00|0.95| |테스트 정확도|0.87|0.96| ## K-최근접이웃 분류기의 개념   K-최근접이웃 분류기(K-Nearest Neighbors)는 새로 주어진 데이터 xnew에 대해, 학습데이터에서 이와 가장 가까운 순서대로 K개의 데이터를 선별하여 후보 데이터집합 N(x)를 만든 뒤, 이 집합에 포함된 각 데이터의 클래스 중 가장 등장빈도가 높은 클래스를 xnew의 클래스로 할당하는 방법이다.   최근접이웃 분류기가 데이터 잡음에 민감하고, 과다적합 문제에 따른 오분류율이 높은 문제를 보완하고자 나온 분류기이다. 최근접이웃 분류기가 xnew 데이터와 `가장 가까운 데이터 하나` 만을 참고했다면, K-최근접이웃 분류기는 xnew 데이터와 `가장 가까운 순으로 k 개의 데이터` 를 참고한다는 점에서 차이가 있다.   ![](/assets/images/20241117_001_004.png)   ### K-최근접이웃 분류기의 수식화   K-최근접이웃 분류기는 학습 데이터 중 xnew와 거리가 가까운 K개의 데이터의 클래스를 확인하고, 이 클래스들 중 가장 등장빈도가 높은 클래스를 xnew에 할당하는 것이다.   ![](/assets/images/20241117_001_005.png)   ### K-최근접이웃 분류기의 처리과정   |순서|단계|설명| |---|---|---| |1|거리 계산|새로운 데이터 x와 모든 학습 데이터 {x1, x2, ... xn}과의 거리를 계산한다.| |2|근접 이웃 식별|거리가 가장 가까운 순서로 K개의 데이터를 찾아, 후보 집합 N(x)를 만든다.| |3|클래스 빈도 탐색|후보 집합 N(x)에 속한 각 데이터들이 속한 클래스들을 확인하고<br>식별된 클래스들 각각의 등장빈도수를 탐색한다.| |4|클래스 할당|3번 단계에서 식별된 가장 빈도수가 높은 클래스를 새로운 데이터 x의 클래스로 할당한다.| ## K-최근접이웃 분류기의 특징   |특징|설명| |---|---| |결정 경계|- 가우시안 베이즈 분류기에 비해 매우 비선형적인 결정경계를 가짐<br>- 데이터 분포 형태에 따라 성능이 크게 좌우되지 않음<br>즉, 비선형적 경계를 갖는 데이터분포에서 좋은 분류성능을 보일 수 있음| |분류 시간 오래걸림|- 새로운 데이터가 주어질 때마다(분류 요청이 있을 때마다) 학습 데이터 전체와 거리 계산 필요<br>- 때문에 계산 시간이 오래 걸리며,<br>- 항상 학습 데이터를 저장하고 있어야 하기 때문에 메모리 소모| ### K-최근접이웃 분류기의 결정경계   K-최근접이웃 분류기는 확률이 아닌 \"데이터\"에 기반한 분류기이다 보니, 데이터 자체의 특성값을 더 잘 반영할 수 있다는 특징이 있다. (이를 바꾸어 말하면 데이터 노이즈에 민감하다는 단점으로도 볼 수 있다.) K-최근접이웃 분류기는 베이즈 분류기보다 매우 비선형적인 결정경계를 가지는 특징이 있으며, 데이터 분포 형태에 따라 성능이 크게 좌우되지 않는다.   예를 들어 위 그래프처럼 데이터들의 경계가 비선형적이라면, 가우시안 베이즈 분류기보다는 KNN의 분류 성능이 더 좋을 수 있다.   ![](/assets/images/20241117_001_006.png)   ### K-최근접이웃 분류기의 한계     KNN 분류모델은 데이터 간 거리를 기반으로 하여 분류를 한다는 원리를 가지고 있다. 여기서 태생적 한계점이 도출되는데, 바로 새로이 분류 대상 데이터가 주어질 때마다 모든 학습데이터와의 거리계산을 수행해야 한다는 문제점이다. 이는 `(1)분류를 할 때마다 계산해야 하므로, 계산량이 많다.` `(2)계산을 위해 항상 학습 데이터를 가지고 있어야 하므로 메모리 소모가 크다.` 라는 두 가지 문제점을 반면 베이즈 분류기의 경우엔 확률밀도함수를 기반으로 분류를 진행한다. 확률밀도함수는 평균과 표준편차에만 의존하므로, 학습시 학습 데이터를 이용해서 일단 파라미터(확률밀도함수)를 추정하고 나면 이후부터는 분류시에 학습데이터가 필요하지 않고, 학습된 파라미터 혹은 평균과 표준편차만을 이용해 분류가 가능하다는 특징이 있다. 즉, 계산량과 메모리상에 KNN보다 장점을 가지고 있다.   때문에 분류 모델을 선정할 때에는 해결해야 하는 문제의 데이터 특성과 개발 환경, 각 분류 방법이 가지는 특성을 파악해서 목적에 맞는 분류기를 선택해야 하는 안목을 길러야 한다.   ## K-최근접이웃 분류기 설계시 고려사항   |고려사항|설명| |---|---| |적절한 K의 값|- K 값(몇 개의 근접이웃)이 너무 작을 경우 노이즈에 민감, 학습 과적합<br>- K 값이 너무 클 경우 데이터 전체적인 비율(선험 확률)에 너무 의존<br>- 적절한 K값으로, 주변의 특성과 전체적인 데이터 특성을 모두 반영하도록 해야함| |거리 함수|- 데이터간 거리 계산 방법은 다양함<br>- 어떤 거리계산 방법을 사용하느냐에 따라 선택되는 이웃이 달라짐<br>- 선택되는 이웃이 달라지면 분류 성능에 직접적인 영향| ### 적절한 K의 값   KNN 모델의 K값에 따라 정확도 즉, 분류기의 성능은 크게 좌우된다. K의 값이 너무 작을 경우엔 근처의 데이터에만 적응하는 과적합 문제가 발생하고, K의 값이 너무 클 경우엔 너무 데이터 전체적인 분포(=선험 확률)에 적응하여 근처의 데이터 특성이 적응되지 않는 문제가 발생할 수도 있다. 따라서 주어진 데이터에 대해 가장 좋은 성능을 보이는 K값을 도출해내는 것은 KNN 모델 구축에 있어 굉장히 중요한 문제이다.   ![](/assets/images/20241117_001_009.png)   <i>예시 : k의 값에 따른 정확도의 차이</i> 적절한 K의 값을 찾는 데에는 여러 방법이 있을 것이지만, 특정한 정답이 있는 것은 아니며, 데이터의 특성과 문제의 특성에 맞는 K값을 찾아가는 것이 중요하다. 아래는 적정 K값을 찾기 위한 대표적인 탐색 방법 두 가지이다.   |K값 탐색 방법|설명| |---|---| |전체 데이터 수 반영|- 전체 데이터 수의 경향성에 맞춰 K 값을 지정하는 방식<br>- 예시 : 전체 데이터 수가 N일 때 `K=N의 제곱근`<br>- 여러 가지 방법 중 하나이며, K 값은 데이터 수 뿐만이 아니라 데이터 분포 특성에도 의존하므로 정답이라고 할 수 없다.| |실험적 검증|- K값에 변화를 줘가면서 실험적으로 어떤 K에서 가장 좋은 성능을 보이는지 측정하는 방법<br>- 예시 : K를 5, 10, 20, 30 으로 변화시키면서 성능 경향성을 파악<br>- 현실적인 방법이며, 따라서 많은 경우에 이 방식으로 K값을 찾는다.<br>- 다만 이 또한 정답은 아니다. 정답은 없다.| ### 거리 함수   ![](/assets/images/20241117_001_007.png)   ![](/assets/images/20241117_001_008.png)   KNN 모델에서 고려해야 할 두 번째는 거리 함수이다. 거리 함수란, 이웃과의 거리를 계산할 때 사용하는 수식 혹은 방법론을 지칭하는 말로 유클리디안 거리, 마할라노비스 거리, 코사인 거리, 노름 등 다양한 거리 함수가 있다. 위 예시에서는 각각 유클리디안 거리, 그리고 맨해튼 거리를 거리함수로 사용했을 때 선택되는 데이터가 달라지는 예시이다.   거리 함수에는 다양한 함수가 있으며, 대표적인 종류는 아래 표를 참고한다.   |번호|거리 함수| |---|---| |1|유클리디안 거리| |2|1차 노름| |3|p차 노름| |4|내적| |5|코사인 거리| |6|정규화 유클리디안 거리| |7|마할라노비스 거리| |...|...|'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Prepare\n",
    "\n",
    "## load test text file\n",
    "test_text_file = test_files[0]\n",
    "f = open(test_text_file, 'r')\n",
    "sentences = f.readlines()\n",
    "f.close()\n",
    "\n",
    "## sentence preprocessing\n",
    "sentences = [y for y in [x.replace('\\n', '') for x in sentences] if y != '']\n",
    "text = ''\n",
    "for sentence in sentences:\n",
    "    text += ' ' + sentence\n",
    "\n",
    "## text\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] JVM DLL not found: /Library/Java/JavaVirtualMachines/jdk-18.0.2.jdk/Contents/Home/lib/libjli.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# extract nouns\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m okt \u001b[38;5;241m=\u001b[39m konlpy\u001b[38;5;241m.\u001b[39mtag\u001b[38;5;241m.\u001b[39mOkt()\n\u001b[1;32m      5\u001b[0m nouns \u001b[38;5;241m=\u001b[39m okt\u001b[38;5;241m.\u001b[39mnouns(text)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# count dictionary\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/keyword_extractor/lib/python3.12/site-packages/konlpy/tag/_okt.py:51\u001b[0m, in \u001b[0;36mOkt.__init__\u001b[0;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, jvmpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_heap_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jpype\u001b[38;5;241m.\u001b[39misJVMStarted():\n\u001b[0;32m---> 51\u001b[0m         jvm\u001b[38;5;241m.\u001b[39minit_jvm(jvmpath, max_heap_size)\n\u001b[1;32m     53\u001b[0m     oktJavaPackage \u001b[38;5;241m=\u001b[39m jpype\u001b[38;5;241m.\u001b[39mJPackage(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkr.lucypark.okt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m     OktInterfaceJavaClass \u001b[38;5;241m=\u001b[39m oktJavaPackage\u001b[38;5;241m.\u001b[39mOktInterface\n",
      "File \u001b[0;32m~/miniconda3/envs/keyword_extractor/lib/python3.12/site-packages/konlpy/jvm.py:64\u001b[0m, in \u001b[0;36minit_jvm\u001b[0;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     61\u001b[0m     jvmpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/lib/jli/libjli.dylib\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m jvmpath\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/lib/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvmpath:\n\u001b[0;32m---> 64\u001b[0m     jpype\u001b[38;5;241m.\u001b[39mstartJVM(jvmpath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-Dfile.encoding=UTF8\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     65\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-ea\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-Xmx\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(max_heap_size),\n\u001b[1;32m     66\u001b[0m                             classpath\u001b[38;5;241m=\u001b[39mclasspath,\n\u001b[1;32m     67\u001b[0m                             convertStrings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify the JVM path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/keyword_extractor/lib/python3.12/site-packages/jpype/_core.py:298\u001b[0m, in \u001b[0;36mstartJVM\u001b[0;34m(jvmpath, classpath, ignoreUnrecognized, convertStrings, interrupt, *jvmargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m prior \u001b[38;5;241m=\u001b[39m [locale\u001b[38;5;241m.\u001b[39mgetlocale(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m categories]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Start the JVM\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m _jpype\u001b[38;5;241m.\u001b[39mstartup(jvmpath, jvmargs \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(extra_jvm_args),\n\u001b[1;32m    299\u001b[0m                ignoreUnrecognized, convertStrings, interrupt)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Collect required resources for operation\u001b[39;00m\n\u001b[1;32m    301\u001b[0m initializeResources()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] JVM DLL not found: /Library/Java/JavaVirtualMachines/jdk-18.0.2.jdk/Contents/Home/lib/libjli.dylib"
     ]
    }
   ],
   "source": [
    "import konlpy\n",
    "\n",
    "# extract nouns\n",
    "okt = konlpy.tag.Okt()\n",
    "nouns = okt.nouns(text)\n",
    "\n",
    "# count dictionary\n",
    "count_dictionary = dict()\n",
    "for noun in nouns:\n",
    "    if noun in count_dictionary.keys():\n",
    "        count_dictionary[noun] += 1\n",
    "    else:\n",
    "        count_dictionary[noun] = 1\n",
    "\n",
    "# count DESC\n",
    "count_desc = sorted(count_dictionary.items(), key=lambda x:x[1], reverse=True)\n",
    "count_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_nouns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keyword_extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
